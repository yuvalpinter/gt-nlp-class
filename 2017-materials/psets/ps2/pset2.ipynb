{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem set 2: Viterbi Sequence Labeling\n",
    "=====================\n",
    "\n",
    "- This project focuses on sequence labeling with Hidden Markov models and implementing the Viterbi algorithm.\n",
    "- The target domain is part-of-speech tagging on the Universal Dependencies dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "from gtnlplib import preproc, viterbi, most_common, clf_base\n",
    "from gtnlplib import naive_bayes, scorer, constants, tagger_base, hmm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# this enables you to create inline plots in the notebook \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Processing\n",
    "\n",
    "The test data will be released around 48 hours before the deadline.\n",
    "The part-of-speech tags are defined on the [universal dependencies website](http://universaldependencies.org/en/pos/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Define the file names\n",
    "TRAIN_FILE = constants.TRAIN_FILE\n",
    "DEV_FILE = constants.DEV_FILE\n",
    "TEST_FILE = constants.TEST_FILE # You do not have this for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is demo code for using the function `conll_seq_generator()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([u'ADV', u'NOUN', u'NUM', u'ADP', u'PRON', u'SCONJ', u'PROPN', u'DET', u'SYM', u'INTJ', u'PART', u'PUNCT', u'VERB', u'X', u'AUX', u'CONJ', u'ADJ'])\n"
     ]
    }
   ],
   "source": [
    "## Demo\n",
    "all_tags = set()\n",
    "for i,(words, tags) in enumerate(preproc.conll_seq_generator(TRAIN_FILE,max_insts=100000)):\n",
    "    for tag in tags:\n",
    "        all_tags.add(tag)\n",
    "print all_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 1.1**: Counting words per tag. (* 0.5 points *)\n",
    "\n",
    "Implement `get_tag_word_counts` in `most_common.py`\n",
    "\n",
    "- **Input**: filename for data file, to be passed as argument to `preproc.conll_seq_generation`\n",
    "- **Output**: dict of counters, where keys are tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(most_common);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADV [(u'so', 371), (u'just', 353), (u'when', 306)]\n",
      "NOUN [(u'time', 385), (u'people', 233), (u'way', 187)]\n",
      "NUM [(u'one', 332), (u'two', 157), (u'2', 129)]\n",
      "ADP [(u'of', 3424), (u'in', 2705), (u'to', 1783)]\n",
      "PRON [(u'I', 3121), (u'you', 1920), (u'it', 1468)]\n",
      "SCONJ [(u'that', 982), (u'if', 450), (u'as', 312)]\n",
      "PROPN [(u'Bush', 211), (u'US', 162), (u'Iraq', 119)]\n",
      "DET [(u'the', 8142), (u'a', 3588), (u'The', 884)]\n",
      "SYM [(u'$', 251), (u'-', 101), (u':)', 34)]\n",
      "INTJ [(u'Please', 141), (u'please', 111), (u'Yes', 34)]\n",
      "PART [(u'to', 3221), (u'not', 805), (u\"n't\", 655)]\n",
      "PUNCT [(u'.', 8645), (u',', 7021), (u'\"', 1352)]\n",
      "VERB [(u'is', 1738), (u'was', 808), (u'have', 748)]\n",
      "X [(u'etc', 39), (u'1', 29), (u'2', 29)]\n",
      "AUX [(u'will', 811), (u'would', 578), (u'can', 578)]\n",
      "CONJ [(u'and', 4843), (u'or', 697), (u'but', 602)]\n",
      "ADJ [(u'other', 268), (u'good', 251), (u'new', 195)]\n"
     ]
    }
   ],
   "source": [
    "# this block uses your code to find the most common words per tag\n",
    "counters = most_common.get_tag_word_counts(TRAIN_FILE)\n",
    "for tag,tag_ctr in counters.iteritems():\n",
    "    print tag,tag_ctr.most_common(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tagging as classification \n",
    "\n",
    "Now you will implement part-of-speech tagging via classification.\n",
    "\n",
    "Tagging quality is evaluated using evalTagger, which takes three arguments:\n",
    "- a tagger, which is a **function** taking a list of words and a tagset as arguments\n",
    "- an output filename\n",
    "- a test file\n",
    "\n",
    "You will want to use lambda expressions to create the first argument for this function, as shown below.\n",
    "Here's how it works. I provide a tagger that labels everything as a noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(tagger_base);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.166891999364\n"
     ]
    }
   ],
   "source": [
    "# here is a tagger that just tags everything as a noun\n",
    "noun_tagger = lambda words, alltags : ['NOUN' for word in words]\n",
    "confusion = tagger_base.eval_tagger(noun_tagger,'nouns',all_tags=all_tags)\n",
    "print scorer.accuracy(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Deliverable 2.1 ** Classification-based tagging. (* 0.5 points *)\n",
    "\n",
    "Now do the same thing, but building your tagger *as a classifier.* To do this, implement `make_classifier_tagger` in `tagger_base.py`. \n",
    "\n",
    "- **Input**: defaultdict of weights\n",
    "- **Output**: function from (list of word tokens, list of all possible tags) --> tags for each word\n",
    "\n",
    "The function that you output should use your predict() function from pset 1. You are free to edit this function if you don't think you got it right in pset 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(tagger_base);\n",
    "reload(clf_base);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier_noun_tagger = tagger_base.make_classifier_tagger(most_common.get_noun_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.166891999364\n"
     ]
    }
   ],
   "source": [
    "confusion = tagger_base.eval_tagger(classifier_noun_tagger,'all-nouns.preds',all_tags=all_tags)\n",
    "print scorer.accuracy(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 2.2** Tagging words by their most common tag. (* 0.5 points *)\n",
    "\n",
    "Now build a classifier tagger that tags each word with its most common tag in the training set. To do this, implement `get_most_common_word_weights` in `most_common.py`.\n",
    "\n",
    "Inputs:\n",
    "\n",
    "- training file\n",
    "\n",
    "Outputs:\n",
    "\n",
    "- defaultdict of weights\n",
    "\n",
    "Each word should get the tag that is most frequently associated with it in the training data. If the word does not appear in the training data, the weights should be set so that the tagger outputs 'NOUN'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(most_common);\n",
    "reload(tagger_base);\n",
    "theta_mc = most_common.get_most_common_word_weights(constants.TRAIN_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagger_mc = tagger_base.make_classifier_tagger(theta_mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'PRON', u'AUX', u'AUX', u'NOUN']\n"
     ]
    }
   ],
   "source": [
    "tags = tagger_mc(['They','can','can','fish'],all_tags)\n",
    "print tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'DET', u'ADJ', u'NOUN', u'DET', u'NOUN', u'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "tags = tagger_mc(['The','old','man','the','boat','.'],all_tags)\n",
    "print tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run your tagger on the dev data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.848496898362\n"
     ]
    }
   ],
   "source": [
    "confusion = tagger_base.eval_tagger(tagger_mc,'most-common.preds',all_tags=all_tags)\n",
    "print scorer.accuracy(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 2.3** Naive Bayes as a tagger. (* 0.5 points *)\n",
    "\n",
    "Now use a Naive Bayes approach to setting the weights for the classifier tagger. For this, you can copy in your ```naive_bayes.py``` from pset1. If you don't think you got it right, you are free to change it now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADJ ADP ADV AUX CONJ DET INTJ NOUN NUM PART PRON PROPN PUNCT SCONJ SYM VERB X\n"
     ]
    }
   ],
   "source": [
    "reload(naive_bayes);\n",
    "sorted_tags = sorted(counters.keys())\n",
    "print ' '.join(sorted_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_weights = naive_bayes.estimate_nb([counters[tag] for tag in sorted_tags],\n",
    "                                     sorted_tags,\n",
    "                                     .01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives weights for each tag-word pair that represent $\\log P(word \\mid tag)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.38657494998 -3.92169758048\n",
      "-3.11635765265 -4.29498560641 -14.4453722986\n",
      "-14.4453722986 -15.0676307485 -1.01587097152\n"
     ]
    }
   ],
   "source": [
    "print nb_weights[('ADJ','bad')], nb_weights[(u'ADJ',u'good')]\n",
    "print nb_weights[(u'PRON',u'they')], nb_weights[(u'PRON',u'They')], nb_weights[(u'PRON',u'good')]\n",
    "print nb_weights[(u'PRON',u'.')], nb_weights[(u'NOUN',u'.')], nb_weights[(u'PUNCT',u'.')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These weights should correspond to probabilities that sum to one for each tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = set([word for tag,word in nb_weights.keys() if word is not constants.OFFSET])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print sum(np.exp(nb_weights[('ADJ',word)]) for word in vocab)\n",
    "print sum(np.exp(nb_weights[('DET',word)]) for word in vocab)\n",
    "print sum(np.exp(nb_weights[('PUNCT',word)]) for word in vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have zero weights for OOV terms -- more on this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print nb_weights[('ADJ','baaaaaaaaad')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As constructed here, the Naive Bayes tagger also does not correctly estimate weights for the offset, $\\log P(tag)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.83321334406\n",
      "-2.83321334406\n",
      "-2.83321334406\n"
     ]
    }
   ],
   "source": [
    "print nb_weights[('VERB'),constants.OFFSET]\n",
    "print nb_weights[('ADV'),constants.OFFSET]\n",
    "print nb_weights[('PRON'),constants.OFFSET]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, the accuracy is not as good as the most-common-tag tagger from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.807181485605\n"
     ]
    }
   ],
   "source": [
    "confusion = tagger_base.eval_tagger(tagger_base.make_classifier_tagger(nb_weights),'nb-simple.preds')\n",
    "dev_acc = scorer.accuracy(confusion)\n",
    "print dev_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 2.4** Fixing the Naive Bayes tagger (* 0.5 points *)\n",
    "\n",
    "Implement ```naive_bayes.estimate_nb_tagger```, which should take two arguments:\n",
    "\n",
    "- A list of word counters for each tag\n",
    "- A smoothing value\n",
    "\n",
    "It should return weights so that \n",
    "\n",
    "- $\\theta[(tag,word)] = \\log P(word \\mid tag)$. If your naive_bayes is correct, it already does this.\n",
    "- $\\theta[(tag,offset)] = \\log P(tag)$. You will need to add some code to make this happen.\n",
    "\n",
    "All probabilities should be smoothed relative frequency estimates. \n",
    "\n",
    "Your implementation should call ```naive_bayes.estimate_nb```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(naive_bayes);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta_nb_fixed = naive_bayes.estimate_nb_tagger(counters,.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# emission weights still sum to one \n",
    "print sum(np.exp(theta_nb_fixed[('ADJ',word)]) for word in vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7.36649959838 -7.36649959838\n"
     ]
    }
   ],
   "source": [
    "# emission weights are identical to theta_nb\n",
    "print nb_weights[('ADJ','okay')],theta_nb_fixed[('ADJ','okay')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.00636615185\n",
      "-2.96505215649\n",
      "-2.39906416857\n"
     ]
    }
   ],
   "source": [
    "# but the offsets are now correct\n",
    "print theta_nb_fixed[('VERB'),constants.OFFSET]\n",
    "print theta_nb_fixed[('ADV'),constants.OFFSET]\n",
    "print theta_nb_fixed[('PRON'),constants.OFFSET]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Offsets should correspond to log-probabilities $\\log P(y)$ such that $\\sum_y P(y) = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99999999999999944"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.exp(theta_nb_fixed[(tag,constants.OFFSET)]) for tag in all_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply the tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.848337839987\n"
     ]
    }
   ],
   "source": [
    "confusion = tagger_base.eval_tagger(tagger_base.make_classifier_tagger(theta_nb_fixed),\n",
    "                                    'nb-fixed.preds')\n",
    "dev_acc = scorer.accuracy(confusion)\n",
    "print dev_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as good as the heuristic tagger from above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Viterbi Algorithm\n",
    "\n",
    "In this section you will implement the Viterbi algorithm. To get warmed up, let's work out an example by hand. For simplicity, there are only two tags, **N**OUN and **V**ERB. Here are the parameters:\n",
    "\n",
    "| | Value |\n",
    "| ------------- |:-------------:|\n",
    "| $\\log P_E(\\cdot|N)$ | they: -1, can: -3, fish: -3 |\n",
    "| $\\log P_E(\\cdot|V)$ | they: -11, can: -2, fish: -4 |\n",
    "| $\\log P_T(\\cdot|N)$ | N: -5, V: -2, END: -2 |\n",
    "| $\\log P_T(\\cdot|V)$ | N: -1, V: -3, END: -3 |\n",
    "| $\\log P_T(\\cdot|\\text{START})$ | N :-1, V :-2 |\n",
    "\n",
    "where $P_E(\\cdot|\\cdot)$ is the emission probability and $P_T(\\cdot|\\cdot)$ is the transition probability.\n",
    " \n",
    "In class we discuss the sentence *They can fish*. Now work out a more complicated example: \"*They can can fish*\", where the second \"*can*\" refers to the verb of putting things into cans.\n",
    " \n",
    "** Deliverable 3.1 ** Work out the trellis by hand, and fill in the table in the file ```text-answers.md``` (*0.5 points*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Viterbi ##\n",
    "\n",
    "Here are some predefined weights, corresponding to the weights from the problem 3.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "START_TAG = constants.START_TAG\n",
    "TRANS = constants.TRANS\n",
    "END_TAG = constants.END_TAG\n",
    "EMIT = constants.EMIT\n",
    "\n",
    "hand_weights = {('NOUN','they',EMIT):-1,\\\n",
    "                ('NOUN','can',EMIT):-3,\\\n",
    "                ('NOUN','fish',EMIT):-3,\\\n",
    "                ('VERB','they',EMIT):-11,\\\n",
    "                ('VERB','can',EMIT):-2,\\\n",
    "                ('VERB','fish',EMIT):-4,\\\n",
    "                ('NOUN','NOUN',TRANS):-5,\\\n",
    "                ('VERB','NOUN',TRANS):-2,\\\n",
    "                (END_TAG,'NOUN',TRANS):-2,\\\n",
    "                ('NOUN','VERB',TRANS):-1,\\\n",
    "                ('VERB','VERB',TRANS):-3,\\\n",
    "                (END_TAG,'VERB',TRANS):-3,\\\n",
    "                ('NOUN',START_TAG,TRANS):-1,\\\n",
    "                ('VERB',START_TAG,TRANS):-2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 3.2** Building HMM features. (*0.5 points*)\n",
    "\n",
    "Implement `hmm_features` in `hmm.py` to compute the HMM features for the function $\\mathbf{f}(\\mathbf{x},y_m,y_{m-1},m)$. \n",
    "\n",
    "Expected behavior is shown below. Note how `constants.EMIT` and `constants.TRANS` are used to distinguish emission and transition features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reload(hmm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['they', 'can', 'can', 'fish']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"they can can fish\".split()\n",
    "print sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('Noun', 'can', '--EMISSION--'): 1, ('Noun', 'Verb', '--TRANS--'): 1}\n",
      "{('Noun', 'they', '--EMISSION--'): 1, ('Noun', '--START--', '--TRANS--'): 1}\n",
      "{('--END--', 'Verb', '--TRANS--'): 1}\n"
     ]
    }
   ],
   "source": [
    "print hmm.hmm_features(sentence,'Noun','Verb',2)\n",
    "print hmm.hmm_features(sentence,'Noun',START_TAG,0)\n",
    "print hmm.hmm_features(sentence,END_TAG,'Verb',4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 3.3** The Viterbi recurrence. (*1 point*)\n",
    "\n",
    "Implement `viterbi_step` in `gtnlplib/viterbi.py`. This is the method that will compute the best path score and corresponding back pointer for a given tag and word, which you will call from the main viterbi routine.\n",
    "\n",
    "- **Input 1**: A tag to calculate the best path for\n",
    "- **Input 2**: An index of the word to calculate the best path for\n",
    "- **Input 3**: A list of words to tag\n",
    "- **Input 4**: A feature function, like hmm_feats\n",
    "- **Input 5**: A dict of weights\n",
    "- **Input 6**: A list of all possible tags\n",
    "- **Input 7**: A list of dicts representing the best scores for the previous trellis layer\n",
    "- **Output 1**: The score of the best-scoring sequence\n",
    "- **Output 2**: The feature in the previous trellis layer corresponding to the best score\n",
    "\n",
    "There are a lot of inputs, but the code itself will not be very complex. Make sure you understand what each input represents before starting to write a solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run your viterbi step on the example in 3.1, by building on the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(viterbi);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-2, '--START--')\n",
      "(-13, '--START--')\n"
     ]
    }
   ],
   "source": [
    "print viterbi.viterbi_step('NOUN',0,sentence,hmm.hmm_features,hand_weights,{START_TAG:0})\n",
    "print viterbi.viterbi_step('VERB',0,sentence,hmm.hmm_features,hand_weights,{START_TAG:0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-10, 'NOUN')\n",
      "(-6, 'NOUN')\n"
     ]
    }
   ],
   "source": [
    "print viterbi.viterbi_step('NOUN',1,sentence,\n",
    "                           hmm.hmm_features,\n",
    "                           hand_weights,\n",
    "                           {'NOUN':-2,'VERB':-13})\n",
    "print viterbi.viterbi_step('VERB',1,sentence,\n",
    "                           hmm.hmm_features,\n",
    "                           hand_weights,\n",
    "                           {'NOUN':-2,'VERB':-13})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 3.4** Build the Viterbi trellis. (*0.5 points*)\n",
    "\n",
    "Use `viterbi_step` to implement `build_trellis` in `viterbi.py`\n",
    "\n",
    "This function should take:\n",
    "\n",
    "- A list of tokens to be tagged\n",
    "- A feature function\n",
    "- A defaultdict of weights\n",
    "- A tag set\n",
    "\n",
    "It should output a list of dicts. In each dict should be key-value pairs of the form `tag:(score,prev_tag)`. See the example output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
